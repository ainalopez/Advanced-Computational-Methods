colnames(new.phi) <- c("x1", "x2", 'y')
# Save the data in a .csv file
if (saveData) {
write.csv(new.phi, file = 'dataset.csv', row.names = FALSE)
cat('Saved file:', paste0(getwd(), '/dataset.csv'), '\n')
}
# Plot
if (savePlot) {
unlink('dataPlot.pdf')
cairo_pdf('dataPlot.pdf')
plot1 <-
ggplot(data = new.phi, aes(x = x1, y = x2,
colour = y, fill = y)) +
geom_point() +
xlab('x1') +
ylab('x2') +
theme_bw()
print(plot1)
dev.off()
cat('Saved plot:', paste0(getwd(), '/dataPlot.pdf'), '\n')
}
# End
return(new.phi)
}  # END OF SCRIPT
if (!require("mvtnorm")) install.packages("mvtnorm")
if (!require("ggplot2")) install.packages("ggplot2")
dataset <- genSun(n = 400, features = 2, seed = 1111, saveData = FALSE, savePlot = FALSE)
features <- as.matrix(cbind(dataset$x1,dataset$x2))
labels   <- as.numeric(as.vector(dataset$y))
# compute KNN
knn.res <- kNN(features, labels, k = 3, p = 1, type = "train")
df <- data.frame(dataset, knn.res)
xmin <- min(dataset$x1); xmax <- max(dataset$x1)
ymin <- min(dataset$x2); ymax <- max(dataset$x2)
x1.n     <-seq(xmin, xmax, len=20)
x2.n     <-seq(ymin, ymax, len=20)
x1.new <-rep(x1.n, 20)
x2.new <-rep(x2.n, rep(20,20))
knn.res.db <- kNN(features, labels, k = 1, p = 1, memory = cbind(x1.new,x2.new), type = "predict")
pred <-knn.res.db$predLabels
ggplot(data = dataset, aes(x = x1, y = x2, colour = labels, fill = labels)) +
geom_point() +
xlab('x1') +
ylab('x2') +
stat_contour( aes(x = x1.new, y = x2.new, z = pred, bins = 1)) +
theme_bw()
if (!require("mvtnorm")) install.packages("mvtnorm")
if (!require("ggplot2")) install.packages("ggplot2")
dataset <- genSun(n = 625, features = 2, seed = 1111, saveData = FALSE, savePlot = FALSE)
features <- as.matrix(cbind(dataset$x1,dataset$x2))
labels   <- as.numeric(as.vector(dataset$y))
# compute KNN
knn.res <- kNN(features, labels, k = 3, p = 1, type = "train")
df <- data.frame(dataset, knn.res)
xmin <- min(dataset$x1); xmax <- max(dataset$x1)
ymin <- min(dataset$x2); ymax <- max(dataset$x2)
x1.n     <-seq(xmin, xmax, len=25)
x2.n     <-seq(ymin, ymax, len=25)
x1.new <-rep(x1.n, 25)
x2.new <-rep(x2.n, rep(25,25))
knn.res.db <- kNN(features, labels, k = 1, p = 1, memory = cbind(x1.new,x2.new), type = "predict")
pred <-knn.res.db$predLabels
ggplot(data = dataset, aes(x = x1, y = x2, colour = labels, fill = labels)) +
geom_point() +
xlab('x1') +
ylab('x2') +
stat_contour( aes(x = x1.new, y = x2.new, z = pred, bins = 1)) +
theme_bw()
ggplot(data = dataset, aes(x = x1, y = x2, colour = labels, fill = labels)) +
geom_point() +
xlab('x1') +
ylab('x2') +
stat_contour( aes(x = x1.new, y = x2.new, z = pred, bins = 10)) +
theme_bw()
training <- read.csv(file = "Desktop/ML_competition/Data/news_popularity_training.csv")
training <- read.csv(file = "Desktop/ML_competition/Data/news_popularity_training.csv")
test     <- read.csv(file = "Desktop/ML_competition/Data/news_popularity_test.csv")
# Popularity 5: less samples
# Keep only predictive data
training.p <- train# ing[c(-1, -2, -3)]
training.p <- training[c(-1, -2, -3)]
test.p     <- test[c(-1, -2, -3)]
training.p <- train
n <- dim(training.p)[1]
training.p <- training[c(-1, -2, -3)]
set.seed(123)
train_ind <- sample(n, size = (n/2+500))
training.p1  <- training.p[train_ind,]
training.p2 <- training.p[-train_ind,-60]
training.p2.label  <- training.p[-train_ind,60]
var(c(1,2,2,1,3,3,1,1,2))
var(c(1,5,5,5,5,5,1,1,1))
noe <- apply(training.p,2,norm<-function(x){return (x/sum(x)})
noe <- apply(training.p,2, function(x){return(x/sum(x)})
noe <- apply(training.p, 2, function(x) (x/sum(x))
{}
noe <- apply(training.p, 2, function(x) (x/sum(x)) )
apply(noe, 2, var)
glm(factor(popularity) ~ . ,
data=training.p1)
lm(factor(popularity) ~ . ,
data=training.p1)
forest <- randomForest(factor(popularity) ~ . ,
data=training.p1,
#mtry = 10,
#sampsize=c(2000,2000,2000,516,k),
cutoff=c(0.125, 0.125, 0.20, 0.25, 0.3),
replace = FALSE,
importance = TRUE, ntree = 300)
library(randomForest)
forest <- randomForest(factor(popularity) ~ . ,
data=training.p1,
#mtry = 10,
#sampsize=c(2000,2000,2000,516,k),
cutoff=c(0.125, 0.125, 0.20, 0.25, 0.3),
replace = FALSE,
importance = TRUE, ntree = 300)
prediction <- predict(forest, training.p2)#testing)
table(prediction, training.p2.label)
dif <- as.numeric(prediction)-training.p2.label#round(svm.pred)-training.p2$popularity
eq  <- 0
for(i in 1:length(dif)){
if(dif[i] == 0){
eq <- eq +1
}
}
dif <- as.numeric(prediction)-training.p2.label#round(svm.pred)-training.p2$popularity
dif
prediction <- predict(forest, training.p2)#testing)
forest
prediction <- predict(forest, training.p2)#testing)
table(prediction, training.p2.label)
length(prediction)
length(training.p2.label)
training.p2.label  <- training.p[-train_ind,60]
training.p2.label
training.p2
training.p <- training[c(-1, -2, -3)]
training.p2.label  <- training.p[-train_ind,60]
training.p2.label
training.p2.label  <- training.p[-train_ind,59]
table(prediction, training.p2.label)
dif <- as.numeric(prediction)-training.p2.label#round(svm.pred)-training.p2$popularity
eq  <- 0
for(i in 1:length(dif)){
if(dif[i] == 0){
eq <- eq +1
}
}
eq/length(dif) *100
training.p11<- training.p1[-c(21,22,22,27,28,29),]
forest <- randomForest(factor(popularity) ~ . ,
data=training.p11,
#mtry = 10,
#sampsize=c(2000,2000,2000,516,k),
cutoff=c(0.125, 0.125, 0.20, 0.25, 0.3),
replace = FALSE,
importance = TRUE, ntree = 300)
training.p22<- training.p2[-c(21,22,22,27,28,29),]
prediction <- predict(forest, training.p22)#testing)
table(prediction, training.p2.label)
# solution <- data.frame(Id = test$id, Popularity = as.numeric(prediction))
# write.csv(solution, file = "predictions_rf.csv", row.names = FALSE)
dif <- as.numeric(prediction)-training.p2.label#round(svm.pred)-training.p2$popularity
eq  <- 0
for(i in 1:length(dif)){
if(dif[i] == 0){
eq <- eq +1
}
}
eq/length(dif) *100
table(prediction, training.p2.label)
length(training.p2.label)
length(prediction)
training.p11<- training.p1[-c(21,22,22,27,28,29)]
str(training.p11)
dim(training.p11)
dim(training.p22)
training.p11<- training.p1[-c(21,22,22,27,28,29)]
training.p22<- training.p2[-c(21,22,22,27,28,29)]
forest <- randomForest(factor(popularity) ~ . ,
data=training.p11,
#mtry = 10,
#sampsize=c(2000,2000,2000,516,k),
cutoff=c(0.125, 0.125, 0.20, 0.25, 0.3),
replace = FALSE,
importance = TRUE, ntree = 300)
# 300: 51.02
# 300 - columna
####predictions####
prediction <- predict(forest, training.p22)#testing)
table(prediction, training.p2.label)
# solution <- data.frame(Id = test$id, Popularity = as.numeric(prediction))
# write.csv(solution, file = "predictions_rf.csv", row.names = FALSE)
dif <- as.numeric(prediction)-training.p2.label#round(svm.pred)-training.p2$popularity
eq  <- 0
for(i in 1:length(dif)){
if(dif[i] == 0){
eq <- eq +1
}
}
eq/length(dif) *100
training.p11<- training.p1[-c(21,22,22,27,28)]
training.p22<- training.p2[-c(21,22,22,27,28)]
forest <- randomForest(factor(popularity) ~ . ,
data=training.p11,
#mtry = 10,
#sampsize=c(2000,2000,2000,516,k),
cutoff=c(0.125, 0.125, 0.20, 0.25, 0.3),
replace = FALSE,
importance = TRUE, ntree = 300)
# 300: 51.02
# 300 - 50.55 columna
####predictions####
prediction <- predict(forest, training.p22)#testing)
table(prediction, training.p2.label)
# solution <- data.frame(Id = test$id, Popularity = as.numeric(prediction))
# write.csv(solution, file = "predictions_rf.csv", row.names = FALSE)
dif <- as.numeric(prediction)-training.p2.label#round(svm.pred)-training.p2$popularity
eq  <- 0
for(i in 1:length(dif)){
if(dif[i] == 0){
eq <- eq +1
}
}
eq/length(dif) *100
training.p11<- training.p1[-c(21,22,22,27)]
training.p22<- training.p2[-c(21,22,22,27)]
forest <- randomForest(factor(popularity) ~ . ,
data=training.p11,
#mtry = 10,
#sampsize=c(2000,2000,2000,516,k),
cutoff=c(0.125, 0.125, 0.20, 0.25, 0.3),
replace = FALSE,
importance = TRUE, ntree = 300)
# 300: 51.02
# 300 - 50.55 columna (29)
# 300 - 50.71 columna (-29)
####predictions####
prediction <- predict(forest, training.p22)#testing)
table(prediction, training.p2.label)
# solution <- data.frame(Id = test$id, Popularity = as.numeric(prediction))
# write.csv(solution, file = "predictions_rf.csv", row.names = FALSE)
dif <- as.numeric(prediction)-training.p2.label#round(svm.pred)-training.p2$popularity
eq  <- 0
for(i in 1:length(dif)){
if(dif[i] == 0){
eq <- eq +1
}
}
eq/length(dif) *100
forest <- randomForest(factor(popularity) ~ . ,
data=training.p1,
#mtry = 10,
#sampsize=c(2000,2000,2000,516,k),
#cutoff=c(0.125, 0.125, 0.20, 0.25, 0.3),
replace = FALSE,
importance = TRUE, ntree = 300)
# 300: 51.02
# 300 - 50.55 columna (29)
# 300 - 50.71 columna -c(21,22,22,27,28)
# 300 -  50.62069columna -c(21,22,22,27)
####predictions####
prediction <- predict(forest, training.p2)#testing)
table(prediction, training.p2.label)
# solution <- data.frame(Id = test$id, Popularity = as.numeric(prediction))
# write.csv(solution, file = "predictions_rf.csv", row.names = FALSE)
dif <- as.numeric(prediction)-training.p2.label#round(svm.pred)-training.p2$popularity
eq  <- 0
for(i in 1:length(dif)){
if(dif[i] == 0){
eq <- eq +1
}
}
eq/length(dif) *100
training.p11<- training.p1[-c(21,22,22,27,28)]
training.p22<- training.p2[-c(21,22,22,27,28)]
forest <- randomForest(factor(popularity) ~ . ,
data=training.p11,
#mtry = 10,
#sampsize=c(2000,2000,2000,516,k),
#cutoff=c(0.125, 0.125, 0.20, 0.25, 0.3),
replace = FALSE,
importance = TRUE, ntree = 300)
# 300: 51.26
# 300 - 50.55 columna (29)
# 300 - 50.71 columna -c(21,22,22,27,28)
# 300 -  50.62069columna -c(21,22,22,27)
####predictions####
prediction <- predict(forest, training.p22)#testing)
table(prediction, training.p2.label)
# solution <- data.frame(Id = test$id, Popularity = as.numeric(prediction))
# write.csv(solution, file = "predictions_rf.csv", row.names = FALSE)
dif <- as.numeric(prediction)-training.p2.label#round(svm.pred)-training.p2$popularity
eq  <- 0
for(i in 1:length(dif)){
if(dif[i] == 0){
eq <- eq +1
}
}
eq/length(dif) *100
training.p11<- training.p1[-c(21)]
training.p22<- training.p2[-c(21)]
forest <- randomForest(factor(popularity) ~ . ,
data=training.p11,
#mtry = 10,
#sampsize=c(2000,2000,2000,516,k),
#cutoff=c(0.125, 0.125, 0.20, 0.25, 0.3),
replace = FALSE,
importance = TRUE, ntree = 300)
# 300: 51.26
# 300 - 50.62 columna -c(21,22,22,27,28)
####predictions####
prediction <- predict(forest, training.p22)#testing)
table(prediction, training.p2.label)
# solution <- data.frame(Id = test$id, Popularity = as.numeric(prediction))
# write.csv(solution, file = "predictions_rf.csv", row.names = FALSE)
dif <- as.numeric(prediction)-training.p2.label#round(svm.pred)-training.p2$popularity
eq  <- 0
for(i in 1:length(dif)){
if(dif[i] == 0){
eq <- eq +1
}
}
eq/length(dif) *100
training.p11<- training.p1[-c(22)]
training.p22<- training.p2[-c(22)]
forest <- randomForest(factor(popularity) ~ . ,
data=training.p11,
#mtry = 10,
#sampsize=c(2000,2000,2000,516,k),
#cutoff=c(0.125, 0.125, 0.20, 0.25, 0.3),
replace = FALSE,
importance = TRUE, ntree = 300)
# 300: 51.26
# 300 - 50.62 columna -c(21,22,23,27,28)
# 300 - 51.17 columna -c(21)
# 300 - 51.17 columna -c(22)
# 300 - 51.17 columna -c(23)
####predictions####
prediction <- predict(forest, training.p22)#testing)
table(prediction, training.p2.label)
# solution <- data.frame(Id = test$id, Popularity = as.numeric(prediction))
# write.csv(solution, file = "predictions_rf.csv", row.names = FALSE)
dif <- as.numeric(prediction)-training.p2.label#round(svm.pred)-training.p2$popularity
eq  <- 0
for(i in 1:length(dif)){
if(dif[i] == 0){
eq <- eq +1
}
}
eq/length(dif) *100
training.p11<- training.p1[-c(23)]
training.p22<- training.p2[-c(23)]
forest <- randomForest(factor(popularity) ~ . ,
data=training.p11,
#mtry = 10,
#sampsize=c(2000,2000,2000,516,k),
#cutoff=c(0.125, 0.125, 0.20, 0.25, 0.3),
replace = FALSE,
importance = TRUE, ntree = 300)
# 300: 51.26
# 300 - 50.62 columna -c(21,22,23,27,28)
# 300 - 51.17 columna -c(21)
# 300 - 50.73 columna -c(22)
# 300 -  columna -c(23)
# 300 -  columna -c(27)
# 300 -  columna -c(28)
# 300 -  columna -c(29)
####predictions####
prediction <- predict(forest, training.p22)#testing)
table(prediction, training.p2.label)
# solution <- data.frame(Id = test$id, Popularity = as.numeric(prediction))
# write.csv(solution, file = "predictions_rf.csv", row.names = FALSE)
dif <- as.numeric(prediction)-training.p2.label#round(svm.pred)-training.p2$popularity
eq  <- 0
for(i in 1:length(dif)){
if(dif[i] == 0){
eq <- eq +1
}
}
eq/length(dif) *100
install.packages("rrf")
install.packages("RRF")
library(RRF)
forest <- rrf(factor(popularity) ~ . ,
data=training.p1,
#mtry = 10,
#sampsize=c(2000,2000,2000,516,k),
#cutoff=c(0.125, 0.125, 0.20, 0.25, 0.3),
replace = FALSE,
importance = TRUE, ntree = 300)
forest <- RRF(factor(popularity) ~ . ,
data=training.p1,
#mtry = 10,
#sampsize=c(2000,2000,2000,516,k),
#cutoff=c(0.125, 0.125, 0.20, 0.25, 0.3),
replace = FALSE,
importance = TRUE, ntree = 300)
forest$importance
str(forest)
forest$confusion
prediction <- predict(forest$forest, training.p22)#testing)
forest$feaSet
training.p11<- training.p1[-c(14, 15, 36)]
training.p22<- training.p2[-c(14,15,36)]
forest <- randomForest(factor(popularity) ~ . ,
data=training.p1,
#mtry = 10,
#sampsize=c(2000,2000,2000,516,k),
#cutoff=c(0.125, 0.125, 0.20, 0.25, 0.3),
replace = FALSE,
importance = TRUE, ntree = 300)
# 300: 51.26
# 300 - 50.62 columna -c(21,22,23,27,28)
# 300 - 51.17 columna -c(21)
# 300 - 50.73 columna -c(22)
# 300 -  50.79 columna -c(23)
# 300 -  columna -c(27)
# 300 -  columna -c(28)
# 300 -  columna -c(29)
####predictions####
prediction <- predict(forest$forest, training.p22)#testing)
table(prediction, training.p2.label)
# solution <- data.frame(Id = test$id, Popularity = as.numeric(prediction))
# write.csv(solution, file = "predictions_rf.csv", row.names = FALSE)
dif <- as.numeric(prediction)-training.p2.label#round(svm.pred)-training.p2$popularity
eq  <- 0
for(i in 1:length(dif)){
if(dif[i] == 0){
eq <- eq +1
}
}
eq/length(dif) *100
prediction <- predict(forest, training.p22)#testing)
forest
prediction <- predict(forest, training.p22)#testing)
library(randomForest)
prediction <- predict(forest, training.p22)#testing)
k
library("class")
setwd("Desktop/Advanced Computational methods/Advanced-Computational-Methods/")
training <- read.csv("PS4/MNIST/MNIST_training.csv", header = FALSE)
test     <- read.csv("PS4/MNIST/MNIST_test.csv", header = FALSE)
label <- training[,1]
feat  <- training[,2:257]
# Choose k and p using 4-Fold Cross Validation
k <- rep(seq(201,501,2))
lk <- length(k)
k <- rep(seq(1,61,2))
lk <- length(k)
# 4 fold Cross-Validation
n <- nrow(training)
fold <- sample(1:4, n, replace=TRUE)
k.max <- 47
predlabel <- knn(train = feat, test = test, cl = label, k = k.ax)
predlabel <- knn(train = feat, test = test, cl = label, k = k.max)
displayDigit(as.numeric(test[3,]), as.numeric(predlabel[3]))
displayDigit <- function(pixels,
label,
predictedLabel = NULL,
saveImage = FALSE,
newDevice = TRUE) {
# converting pixel row into a matrix required by image
img = matrix(pixels, 16, 16, byrow = TRUE)
# image is upside down, need to rotate it for 180 degrees
rotate <- function(x) t(apply(x, 2, rev))
img <- rotate(img)
# do we have predictions as well?
if (is.null(predictedLabel)) predictedLabel <- "-"
# small plot function
plotDigit <- function() {
par(mar = c(0, 0, 2, 0))
image(img, axes = FALSE, col = grey(seq(0, 1, length = 256)))
title(main = paste("Label: ", label, "|  Prediction: ",predictedLabel))
}
# saving image if instructed
if (saveImage) {
png("digit.png")
plotDigit()
dev.off()
}
# plotting
if (newDevice ) dev.new(width=4, height=4.2)
plotDigit()
}
displayDigitSeq <- function(features, labels, predictedLabels = NULL) {
for (digit in 1:nrow(features)) {
# 1. label on first spot
label <- labels[digit]
# 2. the rest are pixel intensities for 16x16 image of digits
pixels <- as.numeric(features[digit, ])
if (is.null(predictedLabels)) {
predictedLabel <- NULL
} else {
predictedLabel <- predictedLabels[digit]
}
# displaying the digit
displayDigit(pixels, label, predictedLabel)
# displaying the next row on key press
key <- readline("Press [enter] to display the next digit ")
if (substr(key, 1, 1) == "b") {dev.off(); break}
dev.off()
}
}
displayDigit(as.numeric(test[3,]), as.numeric(predlabel[3]))
displayDigit(as.numeric(test[1,]), as.numeric(predlabel[1]))
displayDigit(as.numeric(test[2,]), as.numeric(predlabel[2]))
write.csv("MNIST_predictions.csv", predlabel)
getwd()
setwd("PS4/")
write.csv(predlabel, "MNIST_predictions.csv", row.names = FALSE)
