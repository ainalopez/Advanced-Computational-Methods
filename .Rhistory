ggplot(data = dataset, aes(x = x1, y = x2, colour = labels, fill = labels)) +
geom_point() +
xlab('x1') +
ylab('x2') +
stat_contour( aes(x = x1.new, y = x2.new, z = pred), binwidth = 1) +
theme_bw()
kNN <- function(features, labels, k, p, memory = NULL, type="train"){
# Needed packages
if (!require("assertthat")) install.packages("assertthat"); library(assertthat)
if (!require("dplyr")) install.packages("dplyr")
# Verify inputs: assertthat
not_empty(features)
if(is.data.frame(features)) features <- as.matrix(features);
if (type == "train") {
assert_that(nrow(features) == length(labels))
}
is.string(type); assert_that(type %in% c("train", "predict"))
not_empty(labels)
is.count(k)
assert_that(p %in% c(1, 2, Inf))
if (type == "predict") {
assert_that(not_empty(memory) &
ncol(memory) == ncol(features) &
nrow(memory) == length(labels))
}
noObs <- nrow(features)
features <- as.matrix(features)
if (type == "train") {
distMatrix <- matrix(NA, noObs, noObs)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- features[obs,]
probeExpanded <- matrix(probe, nrow = noObs, ncol = 2, byrow = TRUE)
# computing distances between the probe and exemplars
if (p %in% c(1,2)) {
distMatrix[obs, ] <- ( rowSums((abs(features - probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(features - probeExpanded), 1, max)
}
}
} else if (type == "predict") {
noMemory <- nrow(memory)
distMatrix <- matrix(NA, noMemory, noObs)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(memory[obs,])
probeExpanded <- matrix(probe, nrow = noMemory, ncol = 2, byrow = TRUE)
# computing distances between the probe and exemplars in the memory
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(features - probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(features - probeExpanded), 1, max)
}
}
}
# Finding the neighbors: Sort the distances for each point in increasing numerical order
neighbors <- apply(distMatrix, 2, order) %>% t()
# the most frequent class in the k nearest neighbors
prob       <- apply(as.matrix(neighbors[,1:k]), MARGIN = 1, function(x) max(table(labels[x])/k) )
predLabels <- apply(as.matrix(neighbors[,1:k]), MARGIN = 1, function(x) as.integer(names(which.max(table(labels[x]))) ))
return(named = list(predLabels = predLabels, prob = prob))
}
xmin <- min(dataset$x1); xmax <- max(dataset$x1)
ymin <- min(dataset$x2); ymax <- max(dataset$x2)
x1.n     <-seq(xmin, xmax, len=25)
x2.n     <-seq(ymin, ymax, len=25)
x1.new <-rep(x1.n, 25)
x2.new <-rep(x2.n, rep(25,25))
knn.res.db <- kNN(cbind(x1.new,x2.new), labels, k = 1, p = 1, memory = features, type = "predict")
pred <-knn.res.db$predLabels
ggplot(data = dataset, aes(x = x1, y = x2, colour = labels, fill = labels)) +
geom_point() +
xlab('x1') +
ylab('x2') +
stat_contour( aes(x = x1.new, y = x2.new, z = pred), binwidth = 1) +
theme_bw()
ggplot(data = dataset, aes(x = x1, y = x2, colour = as.factor(labels), fill = as.factor(labels))) +
geom_point() +
xlab('x1') +
ylab('x2') +
stat_contour( aes(x = x1.new, y = x2.new, z = pred), binwidth = 1) +
theme_bw()
ggplot(data = dataset, aes(x = x1, y = x2, colour = as.factor(labels), fill = labels )) +
geom_point() +
xlab('x1') +
ylab('x2') +
stat_contour( aes(x = x1.new, y = x2.new, z = pred), binwidth = 1) +
theme_bw()
ggplot(data = dataset, aes(x = x1, y = x2, colour = y, fill = y )) +
geom_point() +
xlab('x1') +
ylab('x2') +
stat_contour( aes(x = x1.new, y = x2.new, z = pred), binwidth = 1) +
theme_bw()
ggplot(data = dataset, aes(x = x1, y = x2, colour = labels, fill = labels )) +
geom_point() +
xlab('x1') +
ylab('x2') +
stat_contour( aes(x = x1.new, y = x2.new, z = pred), binwidth = 1) +
theme_bw()
x1.n     <-seq(xmin, xmax, len=40)
x2.n     <-seq(ymin, ymax, len=40)
x1.new <-rep(x1.n, 20)
x2.new <-rep(x2.n, rep(20,20))
knn.res.db <- kNN(cbind(x1.new,x2.new), labels, k = 1, p = 1, memory = features, type = "predict")
pred <-knn.res.db$predLabels
ggplot(data = dataset, aes(x = x1, y = x2, colour = labels, fill = labels )) +
geom_point() +
xlab('x1') +
ylab('x2') +
stat_contour( aes(x = x1.new, y = x2.new, z = pred), binwidth = 1) +
theme_bw()
xmin <- min(dataset$x1); xmax <- max(dataset$x1)
ymin <- min(dataset$x2); ymax <- max(dataset$x2)
x1.n     <-seq(xmin, xmax, len=100)
x2.n     <-seq(ymin, ymax, len=100)
x1.new <-rep(x1.n, 100)
x2.new <-rep(x2.n, rep(100,100))
knn.res.db <- kNN(cbind(x1.new,x2.new), labels, k = 1, p = 1, memory = features, type = "predict")
pred <-knn.res.db$predLabels
ggplot(data = dataset, aes(x = x1, y = x2, colour = labels, fill = labels )) +
geom_point() +
xlab('x1') +
ylab('x2') +
stat_contour( aes(x = x1.new, y = x2.new, z = pred), binwidth = 1) +
theme_bw()
dataset <- genSun(n = 1000, features = 2, seed = 1111, saveData = FALSE, savePlot = FALSE)
features <- as.matrix(cbind(dataset$x1,dataset$x2))
labels   <- as.numeric(as.vector(dataset$y))
xmin <- min(dataset$x1); xmax <- max(dataset$x1)
ymin <- min(dataset$x2); ymax <- max(dataset$x2)
x1.n     <-seq(xmin, xmax, len=100)
x2.n     <-seq(ymin, ymax, len=100)
x1.new <-rep(x1.n, 100)
x2.new <-rep(x2.n, rep(100,100))
knn.res.db <- kNN(cbind(x1.new,x2.new), labels, k = 1, p = 1, memory = features, type = "predict")
pred <-knn.res.db$predLabels
dataset <- genSun(n = 1000, features = 2, seed = 1111, saveData = FALSE, savePlot = FALSE)
features <- as.matrix(cbind(dataset$x1,dataset$x2))
labels   <- as.numeric(as.vector(dataset$y))
xmin <- min(dataset$x1); xmax <- max(dataset$x1)
ymin <- min(dataset$x2); ymax <- max(dataset$x2)
x1.n     <-seq(xmin, xmax, len=100)
x2.n     <-seq(ymin, ymax, len=100)
x1.new <-rep(x1.n, 100)
x2.new <-rep(x2.n, rep(100,100))
knn.res.db <- kNN(cbind(x1.new,x2.new), labels, k = 1, p = 1, memory = features, type = "predict")
pred <-knn.res.db$predLabels
dataset <- genSun(n = 100, features = 2, seed = 1111, saveData = FALSE, savePlot = FALSE)
features <- as.matrix(cbind(dataset$x1,dataset$x2))
labels   <- as.numeric(as.vector(dataset$y))
xmin <- min(dataset$x1); xmax <- max(dataset$x1)
ymin <- min(dataset$x2); ymax <- max(dataset$x2)
x1.n     <-seq(xmin, xmax, len=10)
x2.n     <-seq(ymin, ymax, len=10)
x1.new <-rep(x1.n, 10)
x2.new <-rep(x2.n, rep(10,10))
knn.res.db <- kNN(cbind(x1.new,x2.new), labels, k = 1, p = 1, memory = features, type = "predict")
pred <-knn.res.db$predLabels
ggplot(data = dataset, aes(x = x1, y = x2, colour = labels, fill = labels )) +
geom_point() +
xlab('x1') +
ylab('x2') +
stat_contour( aes(x = x1.new, y = x2.new, z = pred), binwidth = 1) +
theme_bw()
dataset <- genSun(n = 400, features = 2, seed = 1111, saveData = FALSE, savePlot = FALSE)
features <- as.matrix(cbind(dataset$x1,dataset$x2))
labels   <- as.numeric(as.vector(dataset$y))
xmin <- min(dataset$x1); xmax <- max(dataset$x1)
ymin <- min(dataset$x2); ymax <- max(dataset$x2)
x1.n     <-seq(xmin, xmax, len=20)
x2.n     <-seq(ymin, ymax, len=20)
x1.new <-rep(x1.n, 20)
x2.new <-rep(x2.n, rep(20,20))
knn.res.db <- kNN(cbind(x1.new,x2.new), labels, k = 1, p = 1, memory = features, type = "predict")
pred <-knn.res.db$predLabels
ggplot(data = dataset, aes(x = x1, y = x2, colour = labels, fill = labels )) +
geom_point() +
xlab('x1') +
ylab('x2') +
stat_contour( aes(x = x1.new, y = x2.new, z = pred), binwidth = 1) +
theme_bw()
xmin <- min(dataset$x1); xmax <- max(dataset$x1)
ymin <- min(dataset$x2); ymax <- max(dataset$x2)
x1.n     <-seq(xmin, xmax, len=20)
x2.n     <-seq(ymin, ymax, len=20)
x1.new <-rep(x1.n, 20)
x2.new <-rep(x2.n, rep(20,20))
knn.res.db <- kNN(cbind(x1.new,x2.new), labels, k = 3, p = 1, memory = features, type = "predict")
pred <-knn.res.db$predLabels
ggplot(data = dataset, aes(x = x1, y = x2, colour = labels, fill = labels )) +
geom_point() +
xlab('x1') +
ylab('x2') +
stat_contour( aes(x = x1.new, y = x2.new, z = pred), binwidth = 1) +
theme_bw()
getwd()
setwd("Desktop/Advanced Computational methods/Advanced-Computational-Methods/")
training <- read.csv("/PS4/MNIST/MNIST_training.csv", header = FALSE)
training <- read.csv("PS4/MNIST/MNIST_training.csv", header = FALSE)
test     <- read.csv("PS4/MNIST/MNIST_test.csv", header = FALSE)
k <- seq(1,20)
k
x
x <- c(1,2,3,4)
knn.dist(x, dist.meth = "euclidean", p = 2)
sample(1:4,75,replace=TRUE)
fold <- sample(1:4,75,replace=TRUE)
n <- nrow(training)
fold <- sample(1:4, n, replace=TRUE)
cvpred <- matrix(NA,nrow=n ,ncol=ncol(training))
p <- repc(c(1,2,Inf), 30)
p <- rep(c(1,2,Inf), 30)
p
p1 <- rep(1, 30)
p2 <- rep(2, 30)
p3 <- rep(Inf, 30)
p <- rbind(p1,p2,p3)
p
p <- cbind(p1,p2,p3)
p
p <- cat(p1,p2,p3)
training[,1]
training[1,]
dim(training)
16*16
label <- training[,1]
label
dim(training)
dim(test)
k <- rep(seq(1,30),3)
p1 <- rep(1, 30)
p2 <- rep(2, 30)
p3 <- rep(Inf, 30)
p <- cat(p1,p2,p3)
p
p  <- c(p1,p2,p3)
p
n <- nrow(training)
fold <- sample(1:4, n, replace=TRUE)
fold
'which
''
'
?which
A
a <- c(1,2,3,4,5,6)
which(a, c(1,1,1))
fold == i
i <- 1
fold != i
#setwd("Desktop/Advanced Computational methods/Advanced-Computational-Methods/")
training <- read.csv("PS4/MNIST/MNIST_training.csv", header = FALSE)
test     <- read.csv("PS4/MNIST/MNIST_test.csv", header = FALSE)
label <- training[,1]
feat  <- training[,2:257]
# Choose k and p using 4-Fold Cross Validation
k <- rep(seq(1,30),3)
p1 <- rep(1, 30)
p2 <- rep(2, 30)
p3 <- rep(Inf, 30)
p  <- c(p1,p2,p3)
# 4 fold Cross-Validation
n <- nrow(training)
fold <- sample(1:4, n, replace=TRUE)
acc <- rep (NA, h)
cvpred <- matrix(NA,nrow=n ,ncol=ncol(training))
#setwd("Desktop/Advanced Computational methods/Advanced-Computational-Methods/")
training <- read.csv("PS4/MNIST/MNIST_training.csv", header = FALSE)
test     <- read.csv("PS4/MNIST/MNIST_test.csv", header = FALSE)
label <- training[,1]
feat  <- training[,2:257]
# Choose k and p using 4-Fold Cross Validation
k <- rep(seq(1,30),3)
p1 <- rep(1, 30)
p2 <- rep(2, 30)
p3 <- rep(Inf, 30)
p  <- c(p1,p2,p3)
# 4 fold Cross-Validation
n <- nrow(training)
fold <- sample(1:4, n, replace=TRUE)
acc <- rep (NA, 90)
cvpred <- matrix(NA,nrow=n ,ncol=ncol(training))
h <- 1
ac <- rep(NA,4)
A <- c(1,2,3,5)
a <- c(1,2,3,7)
is.equal(A,a)
sum(A!=a)
l
h
i <- 2
l <- kNN(feat[fold==i, ], label[fold!=i], k[h], p[h], memory = feat[fold!=i, ], type="predict")
l <- kNN(train = feat[fold!=i, ], test = feat[fold==i, ], cl = label[fold!=i], k[h])
l <- knn(train = feat[fold!=i, ], test = feat[fold==i, ], cl = label[fold!=i], k[h])
library("class")
l <- knn(train = feat[fold!=i, ], test = feat[fold==i, ], cl = label[fold!=i], k[h])
l
k <- rep(seq(1,30),3)
#p1 <- rep(1, 30)
#p2 <- rep(2, 30)
#p3 <- rep(Inf, 30)
#p  <- c(p1,p2,p3)
# 4 fold Cross-Validation
n <- nrow(training)
fold <- sample(1:4, n, replace=TRUE)
acc <- rep (NA, 30)
cvpred <- matrix(NA,nrow=n ,ncol=ncol(training))
for (h in 1:30){
ac <- rep(NA,4)
for (i in 1:4){
l <- knn(train = feat[fold!=i, ], test = feat[fold==i, ], cl = label[fold!=i], k[h])
ac[i] <- (1/length()) * sum(label[fold==i] != l)
}
acc[h] <- mean(ac)
}
n <- nrow(training)
fold <- sample(1:4, n, replace=TRUE)
acc <- rep (NA, 30)
cvpred <- matrix(NA,nrow=n ,ncol=ncol(training))
for (h in 1:30){
ac <- rep(NA,4)
for (i in 1:4){
l <- knn(train = feat[fold!=i, ], test = feat[fold==i, ], cl = label[fold!=i], k[h])
ac[i] <- (1/length(l)) * sum(label[fold==i] != l)
}
acc[h] <- mean(ac)
}
k <- rep(seq(1,30,2))
k
l <- knn(train = feat[fold!=i, ], test = feat[fold==i, ], cl = label[fold!=i], k[h])
k <- rep(seq(1,30,2))
#p1 <- rep(1, 30)
#p2 <- rep(2, 30)
#p3 <- rep(Inf, 30)
#p  <- c(p1,p2,p3)
# 4 fold Cross-Validation
n <- nrow(training)
fold <- sample(1:4, n, replace=TRUE)
acc <- rep (NA, 30)
cvpred <- matrix(NA,nrow=n ,ncol=ncol(training))
for (h in 1:30){
ac <- rep(NA,4)
for (i in 1:4){
l <- knn(train = feat[fold!=i, ], test = feat[fold==i, ], cl = label[fold!=i], k[h])
ac[i] <- (1/length(l)) * sum(label[fold==i] != l)
}
acc[h] <- mean(ac)
}
k <- rep(seq(1,30,2))
lk <- length(k)
#p1 <- rep(1, 30)
#p2 <- rep(2, 30)
#p3 <- rep(Inf, 30)
#p  <- c(p1,p2,p3)
# 4 fold Cross-Validation
n <- nrow(training)
fold <- sample(1:4, n, replace=TRUE)
acc <- rep (NA, lk)
cvpred <- matrix(NA,nrow=n ,ncol=ncol(training))
for (h in 1:lk){
ac <- rep(NA,4)
for (i in 1:4){
l <- knn(train = feat[fold!=i, ], test = feat[fold==i, ], cl = label[fold!=i], k[h])
ac[i] <- (1/length(l)) * sum(label[fold==i] != l)
}
acc[h] <- mean(ac)
}
acc
k <- rep(seq(31,100,2))
lk <- length(k)
#p1 <- rep(1, 30)
#p2 <- rep(2, 30)
#p3 <- rep(Inf, 30)
#p  <- c(p1,p2,p3)
# 4 fold Cross-Validation
n <- nrow(training)
fold <- sample(1:4, n, replace=TRUE)
acc <- rep (NA, lk)
cvpred <- matrix(NA,nrow=n ,ncol=ncol(training))
for (h in 1:lk){
ac <- rep(NA,4)
for (i in 1:4){
l <- knn(train = feat[fold!=i, ], test = feat[fold==i, ], cl = label[fold!=i], k[h])
ac[i] <- (1/length(l)) * sum(label[fold==i] != l)*100
}
acc[h] <- mean(ac)
}
training <- read.csv("PS4/MNIST/MNIST_training.csv", header = FALSE)
test     <- read.csv("PS4/MNIST/MNIST_test.csv", header = FALSE)
label <- training[,1]
feat  <- training[,2:257]
# Choose k and p using 4-Fold Cross Validation
k <- rep(seq(1,31,2))
lk <- length(k)
# 4 fold Cross-Validation
n <- nrow(training)
fold <- sample(1:4, n, replace=TRUE)
acc <- rep (NA, lk)
cvpred <- matrix(NA,nrow=n ,ncol=ncol(training))
for (h in 1:lk){
ac <- rep(NA,4)
for (i in 1:4){
l <- knn(train = feat[fold!=i, ], test = feat[fold==i, ], cl = label[fold!=i], k[h])
ac[i] <- (1/length(l)) * sum(label[fold==i] == l)*100
}
acc[h] <- mean(ac)
}
acc
k[1]
l <- knn(train = feat, test = test, cl = label, k[1])
predlabel <- l
displayDigit <- function(pixels,
label,
predictedLabel = NULL,
saveImage = FALSE,
newDevice = TRUE) {
# converting pixel row into a matrix required by image
img = matrix(pixels, 16, 16, byrow = TRUE)
# image is upside down, need to rotate it for 180 degrees
rotate <- function(x) t(apply(x, 2, rev))
img <- rotate(img)
# do we have predictions as well?
if (is.null(predictedLabel)) predictedLabel <- "-"
# small plot function
plotDigit <- function() {
par(mar = c(0, 0, 2, 0))
image(img, axes = FALSE, col = grey(seq(0, 1, length = 256)))
title(main = paste("Label: ", label, "|  Prediction: ",predictedLabel))
}
# saving image if instructed
if (saveImage) {
png("digit.png")
plotDigit()
dev.off()
}
# plotting
if (newDevice ) dev.new(width=4, height=4.2)
plotDigit()
}
displayDigitSeq <- function(features, labels, predictedLabels = NULL) {
for (digit in 1:nrow(features)) {
# 1. label on first spot
label <- labels[digit]
# 2. the rest are pixel intensities for 16x16 image of digits
pixels <- as.numeric(features[digit, ])
if (is.null(predictedLabels)) {
predictedLabel <- NULL
} else {
predictedLabel <- predictedLabels[digit]
}
# displaying the digit
displayDigit(pixels, label, predictedLabel)
# displaying the next row on key press
key <- readline("Press [enter] to display the next digit ")
if (substr(key, 1, 1) == "b") {dev.off(); break}
dev.off()
}
}
displayDigit(train, predlabel, newDevice = FALSE)
displayDigit(test, predlabel, newDevice = FALSE)
predlabel
displayDigit(as.numeric(test), as.numeric(predlabel))
displayDigit(test, as.numeric(predlabel))
displayDigit(test, as.numeric(predlabel),newDevice = FALSE)
displayDigit(test[1, ], as.numeric(predlabel)[1], newDevice = FALSE)
displayDigit(test[1, ], as.numeric(predlabel)[1])
test[1, ]
as.numeric(test[1, ])
displayDigit(as.numeric(test[1, ]), as.numeric(predlabel)[1])
displayDigit(as.numeric(test), as.numeric(predlabel), newDevice = FALSE)
displayDigit(as.numeric(test), as.numeric(predlabel))
displayDigit(as.numeric(test[2,]), as.numeric(predlabel[2]))
displayDigit(as.numeric(test[3,]), as.numeric(predlabel[3]))
library("class")
#setwd("Desktop/Advanced Computational methods/Advanced-Computational-Methods/")
training <- read.csv("PS4/MNIST/MNIST_training.csv", header = FALSE)
test     <- read.csv("PS4/MNIST/MNIST_test.csv", header = FALSE)
label <- training[,1]
feat  <- training[,2:257]
# Choose k and p using 4-Fold Cross Validation
k <- rep(seq(1,31,2))
lk <- length(k)
# 4 fold Cross-Validation
n <- nrow(training)
fold <- sample(1:4, n, replace=TRUE)
acc <- rep (NA, lk)
cvpred <- matrix(NA,nrow=n ,ncol=ncol(training))
for (h in 1:lk){
ac <- rep(NA,4)
for (i in 1:4){
l <- knn(train = feat[fold!=i, ], test = feat[fold==i, ], cl = label[fold!=i], k = k[h])
ac[i] <- (1/length(l)) * sum(label[fold==i] == l)*100
}
acc[h] <- mean(ac)
}
acc
predlabel <- knn(train = feat, test = test, cl = label, k = 1)
displayDigit(as.numeric(test[3,]), as.numeric(predlabel[3]))
displayDigit(as.numeric(test[4,]), as.numeric(predlabel[4]))
displayDigit(as.numeric(test[5,]), as.numeric(predlabel[5]))
displayDigit(as.numeric(test[6,]), as.numeric(predlabel[6]))
displayDigit(as.numeric(test[7,]), as.numeric(predlabel[7]))
h <- 1
l <- 1
i <- 1
l <- knn(train = feat[fold!=i, ], test = feat[fold==i, ], cl = label[fold!=i], k = k[h])
l
label[fold==i]
sum(label[fold==i] == l)
(1/length(l)) * sum(label[fold==i] == l)
(1/length(l)) * sum(label[fold==i] != l)*100
